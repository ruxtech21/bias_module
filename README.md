# bias_module
This bias detection module reads user-defined comments our outputs from other AI systems and classifies if the text is racially biased or not, how confident it is of its decision and a suggestion for how to turn the biased comment into a neutral one. The model is trained on the Jigsaw Unintended Bias in Toxicity Classification dataset, which contains over 2 million biased comments, from subtle to explicit. This repository contains two models, as well as some test scripts to evaluate their performance.

The 'train_model_CUDA_version.py' script is the training script for the first and simpler version of this model. It is trained only on the 'train.csv' file, and it has good performance on simpler and more explicitly biased comments. The 'test_challenging_examples.py', 'test_custom_examples.py' and 'test_real_examples.py' scripts are the test scripts for this simpler version of the bias detection module, each one having examples of different difficulties to really put the model's capabilities to the test. The 'test_challenging_examples.py' and 'test_custom_examples.py' scripts contain texts that are manually generated, while the 'test_real_examples.py' script contains examples taken from the dataset, the model having no access to the label of the comment ('biased' or 'unbiased').

The 'train_model_CUDA_version_multi.py' is the better model, being able to generalize on much more complex examples than its first version. It is trained on four CSVs in comparison to only one, 'train.csv', 'test_private_expanded.csv', 'all_data.csv', 'toxicity_individual_annotations.csv'. Its corresponding test script, 'test_complex_bias.py' compiles 100 manually generated examples, which are grouped into pairs of two: a biased comment and an unbiased / control comment. These comments span 50 different domains in which racial bias may arise, and the text prompts are challenging, displaying different levels of severity, from explicit to very subtle bias. The neutral comments are meant to test the model's ability to distinguish between harmful, hateful statements, and purely informative content that might contain historical or economical facts about people of color. 


